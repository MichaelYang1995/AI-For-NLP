{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepear Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path, jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word2vec_model_path = r'D:\\Assignment\\Project_01\\Word2vec_model.w2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_word_path = r'D:\\Assignment\\Project_01\\related_word.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec.load(Word2vec_model_path).wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = word2vec.vocab.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['此外', '自', '本周', '6', '月', '12', '日起', '除', '小米', '手机']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(key)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('稳定版', 0.9151314496994019),\n",
       " ('乐东黎族自治县', 0.9127017259597778),\n",
       " ('御史大夫', 0.91154944896698),\n",
       " ('范丽青', 0.9077432155609131),\n",
       " ('饰件', 0.9041963815689087),\n",
       " ('陈瑞谢', 0.9034069776535034),\n",
       " ('酥碱', 0.9024229049682617),\n",
       " ('北京火车站', 0.902195155620575),\n",
       " ('长缆', 0.9020404815673828),\n",
       " ('诺氟沙星', 0.8997544050216675)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('体验版', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_word(file_path):\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    content_line = file.readline()\n",
    "    result = []\n",
    "    i = 0\n",
    "    while content_line:\n",
    "        temp = ''\n",
    "        content_line = content_line.strip(\"\\n\")\n",
    "        if len(content_line) > 0:\n",
    "            result.append(content_line)\n",
    "        content_line = file.readline()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_word = get_related_word(related_word_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['说', '指出', '表示', '认为', '坦言', '透露', '看来', '告诉', '提到', '所说']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 SIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get the probality of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(word, model):\n",
    "    keys = model.vocab.keys()\n",
    "    total_words_count = sum([v.count for k, v in model.vocab.items()])\n",
    "    esp = 1 / total_words_count\n",
    "\n",
    "    if word in keys:\n",
    "        word_count = model.vocab[word].count\n",
    "        return word_count / total_words_count\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5933924963048614e-05"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('啊', word2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sentences embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SIF(word, model, a=0.01):\n",
    "    return (a + get_probability(word, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIF_sentence_embdding(sentence, model, a=0.01):\n",
    "    result = 0\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        temp = model[word] * get_SIF(word, model)\n",
    "        result += temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_SIF_sentence_embdding(news, model=word2vec, embedding_size=35, a=0.01):\n",
    "    X = [SIF_sentence_embdding(sentence, model) for sentence in news]\n",
    "    \n",
    "    pca = PCA(n_components=min(embedding_size, len(X)))\n",
    "    \n",
    "    pca.fit(np.array(X))\n",
    "    \n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "    \n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    Y = []\n",
    "    for Vs in X:\n",
    "        sub = np.multiply(u, Vs)\n",
    "        Y.append(np.subtract(Vs, sub))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_PCA_SIF_sentence_embdding(content):\n",
    "    result = []\n",
    "    for news in content:\n",
    "        yield PCA_SIF_sentence_embdding(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_embdding = generate_all_PCA_SIF_sentence_embdding(sentence_word_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2):\n",
    "    return cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 NER & Dependency Prasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'D:\\stanford_nlp', lang='zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 execute program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return ''.join(re.findall(r'[\\d|\\w]+', string))\n",
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(string))\n",
    "def cut_sentence(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_content_with_punctuation(news):\n",
    "    news = re.sub(r'\\n', '', news)\n",
    "    if news != '' and news != ' ':\n",
    "        result = cut_sentence(news)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_content(news_sentence_with_punctuation):\n",
    "    result = []\n",
    "    \n",
    "    for sentence in news_sentence_with_punctuation:\n",
    "        temp = token(sentence)\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_content(news_sentence):\n",
    "    result = []\n",
    "    \n",
    "    for sentence in news_sentence:\n",
    "        temp_word = jieba.lcut(sentence)\n",
    "        result.append(temp_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memo(func):\n",
    "    cache = {}\n",
    "\n",
    "    def _wrap(*args): ## ? *args, **kwargs\n",
    "        if args in cache: result = cache[args]\n",
    "        else:\n",
    "            result = func(*args)\n",
    "            cache[args] = result\n",
    "        return result\n",
    "    return _wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_NER(sentence):\n",
    "    return nlp.ner(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_POS(sentence):\n",
    "    return nlp.pos_tag(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLP_DPR(sentence):\n",
    "    return nlp.dependency_parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SUBJ(string):\n",
    "    return re.search('subj', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memo\n",
    "def judge_pronoun(sentence):\n",
    "    subsentence = re.search('(.+)“|”(.+)', sentence)\n",
    "    if tmp_sen:\n",
    "        sentence = subsentence.group(1)\n",
    "    POS = NLP_POS(sentence)\n",
    "    D_pr = NLP_DPR(sentence)\n",
    "    for relation in D_pr:\n",
    "        i = relation[-1] - 1\n",
    "        if SUBJ(relation[0]) and (POS[i][-1] == 'PN' or POS[i][-1] == 'PRP'):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(subj_index, word_cut, D_pr):\n",
    "    name = word_cut[subj_index]\n",
    "    for relation in D_pr:\n",
    "        if relation[0] == 'appos':continue\n",
    "        if relation[1] - 1 == subj_index:\n",
    "            name = word_cut[relation[2] - 1] + name\n",
    "            print(relation)\n",
    "            print(word_cut[relation[2] - 1])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_saying(predicate_index, word_cut, D_pr):\n",
    "    if ':' in word_cut:\n",
    "        return ''.join(word_cut[word_cut.index(':')+1:])\n",
    "    \n",
    "    return ''.join(word_cut[predicate_index+1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sentence(sentence):\n",
    "    name = ''\n",
    "    saying = ''\n",
    "    NER = NLP_NER(sentence)\n",
    "    word_cut = [w for w, _ in NER]\n",
    "    \n",
    "    #寻找句中第一个与‘说’相关的词汇，若无则直接停止解析，返回False。\n",
    "    say_related_word = [word for word in word_cut if word in related_word]\n",
    "    if not say_related_word: return False\n",
    "    \n",
    "    D_pr = NLP_DPR(sentence)\n",
    "    \n",
    "    for i, relation in enumerate(D_pr):\n",
    "        \n",
    "        #首先进行句式解析，判断依赖‘说’的主语是否为'PERSON/ ORGANIZATION/ LOCATION'。\n",
    "        d = relation[1] - 1 #谓语索引\n",
    "        k = relation[2] - 1 #主语索引\n",
    "        \n",
    "        if (word_cut[d] in say_related_word) and SUBJ(relation[0]):  #找出第一个主谓结构\n",
    "             if (NER[k][-1] == 'PERSON' or NER[k][-1] == 'ORGANIZATION' or NER[k][-1] == 'LOACTION'):\n",
    "                name = get_name(k, word_cut, D_pr)\n",
    "                saying = get_saying(d, word_cut, D_pr)\n",
    "                \n",
    "                if not saying:\n",
    "                    quotations = re.findall(r'“(.+?)”', sentence)\n",
    "                    if quotations: \n",
    "                        saying = quotations[-1]\n",
    "                return name, saying\n",
    "            \n",
    "        #若句子中有与‘说’相关词汇，且存在‘：’，则直取其后言论。\n",
    "        if word_cut[i][0] == ':':\n",
    "            for j in range(j):\n",
    "                if NER[j][-1] == 'PERSON' or NER[j][-1] == 'ORGANIZATION' or NER[j][-1] == 'LOACTION':\n",
    "                    name += NER[j][-1]\n",
    "            saying = ''.join(word_cut[i+1:])\n",
    "            return name, saying\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_sentence(sen1_index, sen2_index, Y):\n",
    "    return cosine(Y[sen1_index], Y[sen2_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_and_point(input_text):\n",
    "    res = []\n",
    "    V_point = defaultdict(list)\n",
    "    flag = -1\n",
    "    \n",
    "    news_sentence_with_punctuation = get_sentence_content_with_punctuation(input_text)\n",
    "    news_sentence = get_sentence_content(news_sentence_with_punctuation)\n",
    "    \n",
    "    news_word_with_punctuation = get_sentence_word_content(news_sentence_with_punctuation)\n",
    "    news_word = get_sentence_word_content(news_sentence)\n",
    "    #print(news_sentence_with_punctuation)\n",
    "    #print(news_sentence)\n",
    "    #print(news_word_with_punctuation)\n",
    "    #print(news_word)\n",
    "    Y = PCA_SIF_sentence_embdding(news_word)\n",
    "    \n",
    "    for j, sentence in enumerate(news_sentence_with_punctuation):\n",
    "        expect = 0.75 #sentence embdding比较因素\n",
    "        print(sentence)\n",
    "        \n",
    "        tmp_sentence_index = \"\" #储存前一个言论的索引\n",
    "        tmp_res = []\n",
    "        #特殊情况处理：当句子中第一个字符出现“\n",
    "        if sentence[0] == '“':\n",
    "            the_subsen_of_people_in == re.search('”(.+)“|”(.+)', sentence)\n",
    "            if the_subsen_of_people_in:\n",
    "                the_subsen_of_people_in = [sen for sen in the_subsen_of_people_in.groups() if sen][0]\n",
    "                \n",
    "                saying = sentence.replace(the_subsen_of_people_in, '') #剩余部分即为言论\n",
    "                if res and judge_pronoun(the_subsen_of_people_in):\n",
    "                    res[-1][1] += saying\n",
    "                else:\n",
    "                    tmp_res = parse_sentence(the_subsen_of_people_in)\n",
    "                    if tmp_res:\n",
    "                        saying += tmp_res[1] if tmp_res else ''\n",
    "                        res.append(tmp_res[0], saying)\n",
    "                continue\n",
    "            elif res:\n",
    "                res[-1][1] += sentence\n",
    "                continue\n",
    "            else:\n",
    "                continue\n",
    "        \n",
    "        #一般情况处理\n",
    "        #提取：发言人，言论内容\n",
    "        tmp_res = parse_sentence(sentence)\n",
    "        \n",
    "        #一般情况处理（1）：不存在发言人\n",
    "        if not tmp_res:\n",
    "            if res and tmp_sentence_index and compare_sentence(tmp_sentence_index, j, Y) > expect:\n",
    "                res[-1][1] += sentence\n",
    "                tmp_sentence_index = j\n",
    "            continue\n",
    "            \n",
    "        #一般情况处理（2）：存在发言人\n",
    "        if tmp_res[-1]:\n",
    "            res.append(tmp_res)\n",
    "        tmp_sentence_index = j\n",
    "    return res\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_and_point_dict(input_text):\n",
    "    res = defaultdict(list)\n",
    "    \n",
    "    tmp = get_person_and_point(input_text)\n",
    "    \n",
    "    for person, saying in tmp:\n",
    "        if saying[0] in ['，', '。', '、', '）', '!', '?']:\n",
    "            saying = saying[1:]\n",
    "        res[person].append(saying)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n"
     ]
    }
   ],
   "source": [
    "if '认为' in related_word:\n",
    "    print('y')\n",
    "else:\n",
    "    print('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '\\u3000\\u3000原标题：叙利亚被“袭机”事件惹怒俄罗斯 警告将瞄准美战机\\n\\u3000\\u3000海外网6月19日电 当地时间6月19日，俄罗斯国防部对美国军方击落叙利亚飞机一事作出反击，宣布停止执行俄美两国签署的“在叙飞行安全备忘录”，并称以后美国领导的国际联军所有的战机，都是俄罗斯军方监控与瞄准的目标，叙利亚局势进一步复杂化。\\n\\u3000\\u3000据纽约时报消息，由于美国军方今日击落了一架叙利亚军机，俄罗斯国防部发布消息，自6月19日起暂停执行俄美间在叙利亚领空“防止空中事件和保障行动期间飞行安全”的相互谅解备忘录。要求美方指挥部对此事件进行彻查，结果与俄方共享。\\n\\u3000\\u3000公告称：“俄空军在叙利亚领空执行任务的地区里，幼发拉底河西岸发现的任何飞行物，包括美国领导的国际联军的飞机和无人机，都将是俄罗斯军方地面和空中防空武器监控与瞄准的目标。”\\n\\u3000\\u3000据叙利亚军方声明，当地时间6月19日，一架政府军机正前往拉卡（Raqqa）市，准备对盘踞于此的IS武装分子进行打击，却突然遭到美军袭击，飞行员至今失踪。声明称：“这次袭击发生的时机，是在叙利亚政府及其盟国的军队在与IS恐怖分子的战斗中获得优势的情况下发生的，本来这些恐怖分子已经在叙利亚的沙漠中节节败退。”\\n\\u3000\\u3000此次“袭机”事件“惹怒”了俄罗斯，俄罗斯参议院国防委员会副主席弗朗茨·克莱琴谢夫（Frants Klintsevich）称美军的行动是“挑衅行为”，实际上是对叙利亚的“军事侵略”。\\n\\u3000\\u3000该部门认为，美军“故意不履行双方2015年签署的“安全备忘录”中规定的义务，因此宣布暂停与美军在该框架下的合作。据报道，该协议一直是美俄两国军队协调在该地区的军事活动的关键。俄罗斯、美国、叙利亚、土耳其等国家在叙利亚的诉求经常是相冲突的，该协议就在其中起到调和作用。在特朗普四月下令袭击叙利亚空军之后，俄方表示将暂停协议，不过几个星期之后又重启，这次时隔两月后再被中断。\\n\\u3000\\u3000俄罗斯外长拉夫罗夫在回应记者时也表示，“涉及到叙利亚地面所发生的事情，毫无疑问，我们认为有必要尊重叙利亚的主权和领土完整，这是联合国2254号决议和其他文件规定的。因此，任何地面行动，包括实施军事行动的参与方，需得到大马士革的许可。”（编译/海外网 杨佳）\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "　　原标题：叙利亚被“袭机”事件惹怒俄罗斯 警告将瞄准美战机　　海外网6月19日电 当地时间6月19日，俄罗斯国防部对美国军方击落叙利亚飞机一事作出反击，宣布停止执行俄美两国签署的“在叙飞行安全备忘录”，并称以后美国领导的国际联军所有的战机，都是俄罗斯军方监控与瞄准的目标，叙利亚局势进一步复杂化。\n",
      "　　据纽约时报消息，由于美国军方今日击落了一架叙利亚军机，俄罗斯国防部发布消息，自6月19日起暂停执行俄美间在叙利亚领空“防止空中事件和保障行动期间飞行安全”的相互谅解备忘录。\n",
      "要求美方指挥部对此事件进行彻查，结果与俄方共享。\n",
      "　　公告称：“俄空军在叙利亚领空执行任务的地区里，幼发拉底河西岸发现的任何飞行物，包括美国领导的国际联军的飞机和无人机，都将是俄罗斯军方地面和空中防空武器监控与瞄准的目标。”\n",
      "　　据叙利亚军方声明，当地时间6月19日，一架政府军机正前往拉卡（Raqqa）市，准备对盘踞于此的IS武装分子进行打击，却突然遭到美军袭击，飞行员至今失踪。\n",
      "声明称：“这次袭击发生的时机，是在叙利亚政府及其盟国的军队在与IS恐怖分子的战斗中获得优势的情况下发生的，本来这些恐怖分子已经在叙利亚的沙漠中节节败退。”\n",
      "　　此次“袭机”事件“惹怒”了俄罗斯，俄罗斯参议院国防委员会副主席弗朗茨·克莱琴谢夫（Frants Klintsevich）称美军的行动是“挑衅行为”，实际上是对叙利亚的“军事侵略”。\n",
      "('parataxis:prnmod', 18, 21)\n",
      "Klintsevich\n",
      "　　该部门认为，美军“故意不履行双方2015年签署的“安全备忘录”中规定的义务，因此宣布暂停与美军在该框架下的合作。\n",
      "据报道，该协议一直是美俄两国军队协调在该地区的军事活动的关键。\n",
      "俄罗斯、美国、叙利亚、土耳其等国家在叙利亚的诉求经常是相冲突的，该协议就在其中起到调和作用。\n",
      "在特朗普四月下令袭击叙利亚空军之后，俄方表示将暂停协议，不过几个星期之后又重启，这次时隔两月后再被中断。\n",
      "　　俄罗斯外长拉夫罗夫在回应记者时也表示，“涉及到叙利亚地面所发生的事情，毫无疑问，我们认为有必要尊重叙利亚的主权和领土完整，这是联合国2254号决议和其他文件规定的。\n",
      "因此，任何地面行动，包括实施军事行动的参与方，需得到大马士革的许可。”\n",
      "（编译/海外网 杨佳）\n"
     ]
    }
   ],
   "source": [
    "V_point = get_person_and_point_dict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'Klintsevich弗朗茨·克莱琴谢夫': ['美军的行动是“挑衅行为”，实际上是对叙利亚的“军事侵略”。'],\n",
       "             '罗夫': ['“涉及到叙利亚地面所发生的事情，毫无疑问，我们认为有必要尊重叙利亚的主权和领土完整，这是联合国2254号决议和其他文件规定的。']})"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Prepear Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path, jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_content_path = r'D:\\Assignment\\Project_01\\news-word-cut.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_word_path = r'D:\\Assignment\\Project_01\\related_word.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_word_list(file_path):\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    content_line = file.readline()\n",
    "    article_contents_list = []\n",
    "    i = 0\n",
    "    while content_line:\n",
    "        temp = ''\n",
    "        content_line = content_line.strip(\"\\n\")\n",
    "        if len(content_line) > 0:\n",
    "            for word in content_line:\n",
    "                if word == ' ':\n",
    "                    article_contents_list += [temp]\n",
    "                    temp = ''\n",
    "                else:\n",
    "                    temp = temp + word\n",
    "        content_line = file.readline()\n",
    "        if temp != '': article_contents_list += [temp]\n",
    "    return article_contents_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_content = generate_word_list(word_content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['此外', '自', '本周', '6', '月', '12', '日起', '除', '小米', '手机']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_content[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = gensim.models.word2vec.Word2Vec.load(\"./Word2vec_model.w2v\").wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('稳定版', 0.9151314496994019),\n",
       " ('乐东黎族自治县', 0.9127017259597778),\n",
       " ('御史大夫', 0.91154944896698),\n",
       " ('范丽青', 0.9077432155609131),\n",
       " ('饰件', 0.9041963815689087),\n",
       " ('陈瑞谢', 0.9034069776535034),\n",
       " ('酥碱', 0.9024229049682617),\n",
       " ('北京火车站', 0.902195155620575),\n",
       " ('长缆', 0.9020404815673828),\n",
       " ('诺氟沙星', 0.8997544050216675)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('体验版', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_word(file_path):\n",
    "    file = open(file_path, \"r\", encoding=\"utf-8\")\n",
    "    content_line = file.readline()\n",
    "    result = []\n",
    "    i = 0\n",
    "    while content_line:\n",
    "        temp = ''\n",
    "        content_line = content_line.strip(\"\\n\")\n",
    "        if len(content_line) > 0:\n",
    "            result.append(content_line)\n",
    "        content_line = file.readline()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_word = get_related_word(related_word_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['说', '指出', '表示', '认为', '坦言', '透露', '看来', '告诉', '提到', '所说']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 SIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Get the probality of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_dict = Counter(word_content)\n",
    "\n",
    "word_list_len = len(word_content)\n",
    "\n",
    "def get_unigrams_probability(word):\n",
    "    esp = 1 / word_list_len\n",
    "    if word in unigrams_dict:\n",
    "        return unigrams_dict[word] / word_list_len \n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5933924963048614e-05"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_unigrams_probability('啊')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Sentences embdding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SIF(word, a=0.01):\n",
    "    return (a + get_unigrams_probability(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SIF_sentence_embdding(sentence, a=0.01):\n",
    "    result = 0\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        temp = word2vec[word] * get_SIF(word)\n",
    "        result += temp\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_SIF_sentence_embdding(news, embedding_size=35, a=0.01):\n",
    "    X = [SIF_sentence_embdding(sentence) for sentence in news]\n",
    "    \n",
    "    pca = PCA(n_components=min(embedding_size, len(X)))\n",
    "    \n",
    "    pca.fit(np.array(X))\n",
    "    \n",
    "    u = pca.components_[0]  # the PCA vector\n",
    "    u = np.multiply(u, np.transpose(u))  # u x uT\n",
    "    \n",
    "    # pad the vector?  (occurs if we have less sentences than embeddings_size)\n",
    "    if len(u) < embedding_size:\n",
    "        for i in range(embedding_size - len(u)):\n",
    "            u = np.append(u, 0)  # add needed extension for multiplication below\n",
    "    # resulting sentence vectors, vs = vs -u x uT x vs\n",
    "    Y = []\n",
    "    for Vs in X:\n",
    "        sub = np.multiply(u, Vs)\n",
    "        Y.append(np.subtract(Vs, sub))\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_all_PCA_SIF_sentence_embdding(content):\n",
    "    result = []\n",
    "    for news in content:\n",
    "        yield PCA_SIF_sentence_embdding(news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sentence_embdding = generate_all_PCA_SIF_sentence_embdding(sentence_word_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2):\n",
    "    return cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 NER & Dependency Prasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP(r'D:\\stanford_nlp', lang='zh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 execute program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return ''.join(re.findall(r'[\\d|\\w]+', string))\n",
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(string))\n",
    "def cut_sentence(para):\n",
    "    para = re.sub('([。！？\\?])([^”’])', r\"\\1\\n\\2\", para)  # 单字符断句符\n",
    "    para = re.sub('(\\.{6})([^”’])', r\"\\1\\n\\2\", para)  # 英文省略号\n",
    "    para = re.sub('(\\…{2})([^”’])', r\"\\1\\n\\2\", para)  # 中文省略号\n",
    "    para = re.sub('([。！？\\?][”’])([^，。！？\\?])', r'\\1\\n\\2', para)\n",
    "    # 如果双引号前有终止符，那么双引号才是句子的终点，把分句符\\n放到双引号后，注意前面的几句都小心保留了双引号\n",
    "    para = para.rstrip()  # 段尾如果有多余的\\n就去掉它\n",
    "    # 很多规则中会考虑分号;，但是这里我把它忽略不计，破折号、英文双引号等同样忽略，需要的再做些简单调整即可。\n",
    "    return para.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_content_with_punctuation(news):\n",
    "    news = re.sub(r'\\n', '', news)\n",
    "    if news != '' and news != ' ':\n",
    "        result = cut_sentence(news)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_content(news_sentence_with_punctuation):\n",
    "    result = []\n",
    "    \n",
    "    for sentence in news_sentence_with_punctuation:\n",
    "        temp = token(sentence)\n",
    "        result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_word_content(news_sentence):\n",
    "    result = []\n",
    "    \n",
    "    for sentence in news_sentence:\n",
    "        temp_word = jieba.lcut(sentence)\n",
    "        result.append(temp_word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_and_point(input_text):\n",
    "    PERSON = []\n",
    "    V_point = defaultdict(list)\n",
    "    flag = -1\n",
    "    \n",
    "    news_sentence_with_punctuation = get_sentence_content_with_punctuation(input_text)\n",
    "    news_sentence = get_sentence_content(news_sentence_with_punctuation)\n",
    "    \n",
    "    news_word_with_punctuation = get_sentence_word_content(news_sentence_with_punctuation)\n",
    "    news_word = get_sentence_word_content(news_sentence)\n",
    "    \n",
    "    for j, sentence in enumerate(news_word_with_punctuation):\n",
    "        for i, word in enumerate(sentence):\n",
    "            if word in related_word:\n",
    "                flag = i\n",
    "                break\n",
    "\n",
    "        if flag >= 0:\n",
    "            Y = PCA_SIF_sentence_embdding(news_word)\n",
    "            \n",
    "            temp_person = ''\n",
    "            temp_sen = news_sentence_with_punctuation[j]\n",
    "            temp_point = [temp_sen]\n",
    "            \n",
    "            NER = nlp.ner(temp_sen)\n",
    "            D_pr = nlp.dependency_parse(temp_sen)\n",
    "            \n",
    "            for i, relation in enumerate(D_pr):\n",
    "                if relation[1] == flag+1 and relation[0] == 'nsubj' and (NER[relation[-1] - 1][-1] == 'PERSON' or NER[relation[-1] - 1][-1] == 'ORGANIZATION'):\n",
    "                    temp_person = sentence[relation[-1] - 1]\n",
    "                    PERSON.append(sentence[relation[-1] - 1])\n",
    "                    for m in range((len(news_word_with_punctuation) - j -1)):\n",
    "                        m += (j+1)\n",
    "                        if(distance(Y[j], Y[m])) < 0.2:\n",
    "                            temp_point.append(news_sentence_with_punctuation[j])\n",
    "                        else:\n",
    "                            break\n",
    "            if temp_person != '':\n",
    "                V_point[temp_person] = temp_point\n",
    "        flag = -1\n",
    "    return PERSON, V_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '（原标题：44岁女子跑深圳约会网友被拒，暴雨中裸身奔走……）\\n@深圳交警微博称：昨日清晨交警发现有一女子赤裸上身，行走在南坪快速上，期间还起了轻生年头，一辅警发现后赶紧为其披上黄衣，并一路劝说她。\\n那么事发时\\n到底都发生了些什么呢？\\n南都记者带您一起还原现场\\n南都记者在龙岗大队坂田中队见到了辅警刘青（发现女生的辅警），一位外表高大帅气，说话略带些腼腆的90后青年。\\n刘青介绍，6月16日早上7时36分，他正在环城南路附近值勤，接到中队关于一位女子裸身进入机动车可能有危险的警情，随后骑着小铁骑开始沿路寻找，大概花了十多分钟在南坪大道坂田出口往龙岗方向的逆行辅道上发现该女子。\\n女子身上一丝不挂地逆车流而行，时走时停，时坐时躺，险象环生。刘青停好小铁骑，和另外一名巡防员追了上去，发现女子的情绪很低落，话不多，刘青尝试和女子交流，劝说女子离开，可女子并不愿意接受，继续缓慢地往南坪快速路的主干道上走去。\\n此时路边上已经聚集了很市民围观，为了不刺激女子的情绪，刘青和巡防员一边盯着女子一边驱赶着围观的群众。\\n现场还原\\n从警方提供的一份视频了解到，16日早上7时25分，女子出现在坂雪岗大道与环城南路的监控视频中，此时女子还穿着白色的内裤，正沿着坂雪岗大道往南坪快速的方向缓慢地走着。\\n当时正值上班高峰期，十字路口的车流已经排起了长队。当女子出现时，路上的市民纷纷驻足观望，不少车辆也放慢了速度，但女子并不为市民观望停下脚步，依然缓慢走着。当女子行进到十字路口中间时，一辆大货车挡住了镜头，但是当女子再次出现镜头时，可以发现女子已经没穿内裤了，全身裸露继续朝着南坪快速方向走去。记者发现，视频中女子周围并没有人尾随或者上前劝止的市民。\\n一大清早路上看到这样的情况\\n恐怕大家都没办法淡定\\n面对这一情况\\n刘青表示，“一开始根本不敢看她，心里挺别扭，感觉很尴尬”，但当刘青跟随女子上了南坪快速路主干道时，女子作出了让人意想不到的举动，她突然靠近护栏要从上面跳下去，刘青赶忙冲上去拉住了女子的手，将其控制住并远离护栏。碍于女子没有穿衣服，刘青递上衣服，女子没接受还把衣服扔到排水沟里，继续往前走，没办法刘青只能紧紧拉着她的一只手跟在后面。\\n刘青一路上耐心地开导安慰她，但只听到她不断地重复着一句话“要是你也遭遇我的事，你也会这样的”，期间她还不时试图挣脱刘青的手要冲向护栏往下跳。\\n就这样，我被牵着走了大概十多分钟，天突然下起了大暴雨，雨大的连眼睛都睁不开”刘青继续说着，瞬间他们就被雨透了，但女子依然不愿意接受刘青的帮助，就继续冒着大雨往前走。\\n大概走了有四十分钟吧，女子突然停下来说“我想回家了”，然后女子也接受了刘青递过来的小黄衣，就出现了深圳微博上的照片，女子披着小黄衣，刘青小心翼翼地在旁边走着的场景。从南平快速下来后，刘青和巡防员将女子带到了附近的坂田派出所。\\n那姑娘到底是遭遇了什么样的事情\\n才会说\\n“要是你也遭遇我的事，你也会这样”\\n据警方透露，该女子姓陈，系湖北人，今年44岁，据家属反映其有精神病史。三天前，陈某从老家来深圳约会网友，但约会受挫导致情绪异常，女子遂产生轻生念头。\\n目前\\n陈某已经被送往深圳某精神病医院进行治疗\\n大大君只希望姑娘能早点康复\\n其实真爱的到来并不存在年龄的限制\\n你们说呢？\\n因善良的原因\\n一众网友纷纷为\\n交警暖男点ZAN\\n@弓常yan桦：就想问这个小哥哥有女票吗\\n@原谅我这一辈子浪荡不羁爱萨摩耶：有什么过不去的要轻生嘛？ 想想自己的家人。同时也感谢交警蜀黍\\n@火心聆听心灵：点赞交警\\n@中華云盾：警察……警察就是群众最需时申出援手\\n@Tomchlee：蜀黍帅！\\n@SJ-李赫海i：这个交警很暖有木有！\\n男子迷奸网友拍418个视频 女方从20岁到50岁不等\\n去年6月7号上午，淮安市涟水县公安局刑警大队突然接到了一个奇怪的报警电话，一名女子言语不清，声称自己遭到了侵害。女子、被侵害、言语不清，几个关键词令接到电话的民警瞬间紧张起来。\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PERSON, V_point = get_person_and_point(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['刘青']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PERSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'刘青': ['刘青介绍，6月16日早上7时36分，他正在环城南路附近值勤，接到中队关于一位女子裸身进入机动车可能有危险的警情，随后骑着小铁骑开始沿路寻找，大概花了十多分钟在南坪大道坂田出口往龙岗方向的逆行辅道上发现该女子。']})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
